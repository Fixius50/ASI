Crear  LLM  
Hecho  por:  Roberto  Monedero  Alonso   
Parte  1:  Teor√≠a  Fundamental  (Actualizada)  
Antes  de  ejecutar  c√≥digo,  entendamos  qu√©  estamos  haciendo  bas√°ndonos  en  tu  
documentaci√≥n
 
y
 
el
 
estado
 
del
 
arte
 
actual.
 
1.  ¬øQu√©  es  "Crear"  un  LLM  localmente?  No  estamos  "creando"  un  cerebro  desde  cero  (Pre-entrenamiento),  lo  cual  requerir√≠a  miles  
de
 
GPUs.
 
Lo
 
que
 
haremos
 
es
 
Fine-Tuning
 
(Ajuste
 
Fino)
.
 
‚óè  Concepto:  Tomas  un  modelo  "base"  que  ya  sabe  hablar  (como  Llama  3  o  Qwen)  y  
lo
 
especializas
 
en
 
una
 
tarea
 
concreta.
 ‚óè  Analog√≠a:  Si  el  modelo  base  es  un  estudiante  universitario  graduado,  el  Fine-Tuning  
es
 
enviarlo
 
a
 
hacer
 
un
 
m√°ster
 
en
 
una
 
especialidad
 
(ej.
 
Cocina
 
o
 
Ingenier√≠a
 
Espacial).
 
2.  La  T√©cnica:  LoRA  (Low-Rank  Adaptation)  Entrenar  todos  los  par√°metros  de  un  modelo  es  costoso.  Usaremos  LoRA  (o  QLoRA  si  
cuantizamos).
 
‚óè  C√≥mo  funciona:  En  lugar  de  modificar  todo  el  "cerebro"  del  modelo,  congelamos  el  
modelo
 
original
 
y
 
entrenamos
 
solo
 
unas
 
peque√±as
 
capas
 
de
 
"adaptadores"
 
encima.
 ‚óè  Ventaja:  Es  mucho  m√°s  r√°pido,  consume  menos  memoria  y  el  resultado  pesa  
megabytes
 
en
 
lugar
 
de
 
gigabytes.
 
3.  El  Flujo  de  Datos  (Pipeline)  El  proceso  sigue  estos  4  pasos  cr√≠ticos:  
1.  Dataset  (Datos):  La  calidad  es  reina  ("Garbage  in,  garbage  out").  Necesitamos  
pares
 
de
 Instrucci√≥n  ->  Respuesta.  2.  Entrenamiento  (Training):  El  Mac  leer√°  los  datos  y  ajustar√°  los  adaptadores  LoRA  
para
 
minimizar
 
el
 
error
 
en
 
sus
 
respuestas.
 3.  Fusi√≥n  (Fusing):  Unimos  los  adaptadores  entrenados  con  el  modelo  base  para  
tener
 
un
 
solo
 
archivo
 
funcional.
 4.  Cuantizaci√≥n  (GGUF):  Convertimos  el  modelo  a  formato  GGUF  (formato  universal  
para
 
CPUs
 
y
 
Mac)
 
y
 
reducimos
 
su
 
precisi√≥n
 
(ej.
 
a
 
4
 
bits)
 
para
 
que
 
ocupe
 
menos
 
espacio
 
sin
 
perder
 
mucha
 
inteligencia.
 
 
Parte  2:  Pr√°ctica  (El  Script  Maestro)  
He  consolidado  todos  los  pasos  manuales  de  los  PDFs  en  un  √∫nico  script  de  Python  
moderno.
 
Este
 
script
 
detectar√°
 
tu
 
entorno,
 
preparar√°
 
los
 
datos,
 
entrenar√°
 
el
 
modelo
 
usando
 
la
 
librer√≠a
 mlx,  lo  fusionar√°  y  lo  convertir√°  a  GGUF  para  LM  Studio.  
Requisitos  Previos  

Abre  tu  terminal  en  el  Mac/Linux/Windows  y  aseg√∫rate  de  tener  lo  siguiente:  
1.  Python  instalado.  2.  Instala  las  librer√≠as  necesarias:  
python3  -m  venv  venv;  source  venv/bin/activate;  pip  install  mlx  mlx-lm  huggingface_hub  ‚Üí  
Mac
 pip  install  torch  transformers  peft  trl  bitsandbytes  accelerate  ‚Üí  Windows/Linux   (Nota  1:  No  necesitamos  instalar  llama.cpp v√≠a  pip,  el  script  clonar√°  y  compilar√°  la  √∫ltima  
versi√≥n
 
autom√°ticamente
 
para
 
asegurar
 
compatibilidad
 
con
 
GGUF).
 
(Nota  2:  Windows  es  m√°s  exigente  con  las  versiones.  Si  el  script  te  da  error  al  instalar  bitsandbytes,  usa  este  comando  espec√≠fico  antes  de  correr  el  script:  pip  install  
https://github.com/jllllll/bitsandbytes-windows-webui/releases/downlo
ad/wheels/bitsandbytes-0.41.1-py3-none-win_amd64.whl (Esta  es  una  versi√≥n  
precompilada
 
para
 
Windows
 
muy
 
popular).)
 
El  Script:  crear_llm_mac.py   nano  crear_llm_mac.py   Copia  este  c√≥digo,  gu√°rdalo  como  crear_llm_mac.py en  una  carpeta  vac√≠a  y  ejec√∫talo:  
import  os  import  json  import  subprocess  import  sys  import  platform  import  shutil  from  pathlib  import  Path  #  Importamos  esto  para  la  auto-reparaci√≥n  del  diccionario  al  final  from  huggingface_hub  import  snapshot_download   #  ==========================================  #  ‚öô  CONFIGURACI√ìN  (Puedes  editar  esto)  #  ==========================================   #  Nombre  de  tu  proyecto  (as√≠  se  llamar√°n  los  archivos  finales)  NOMBRE_PROYECTO  =  "Mi-IA-Universal"   #  Modelo  base:  Qwen  2.5  (3B)  es  excelente  para  Mac:  r√°pido  y  muy  inteligente.  MODELO_ID  =  "Qwen/Qwen2.5-3B-Instruct"   #  System  Prompt:  La  "personalidad"  base  de  tu  IA.  SYSTEM_PROMPT  =  "Eres  un  asistente  inteligente  y  conciso."   #  TUS  DATOS:  Aqu√≠  es  donde  la  IA  aprende.  

#  Para  el  ejemplo  usamos  pocos,  pero  para  una  IA  real,  pon  aqu√≠  50-100  ejemplos.  EJEMPLOS_ENTRENAMIENTO  =  [      {"user":  "¬øCapital  de  Espa√±a?",  "assistant":  "Madrid."},      {"user":  "Explica  la  gravedad.",  "assistant":  "Es  la  fuerza  que  atrae  objetos  con  masa."},      {"user":  "Python:  print  hola",  "assistant":  "print('hola')"},      {"user":  "¬øQui√©n  eres?",  "assistant":  "Soy  una  IA  personalizada  entrenada  en  tu  Mac."}  ]   #  Configuraci√≥n  t√©cnica  TRAIN_EPOCHS  =  100         #  Iteraciones.  100  es  r√°pido  para  probar.  Pon  600+  para  
aprender
 
de
 
verdad.
 BATCH_SIZE  =  1             #  Dejar  en  1  para  no  saturar  la  memoria  del  Mac.  TIPO_CUANTIZACION  =  "q4_k_m"  #  El  formato  de  compresi√≥n  equilibrado.   #  Rutas  del  sistema  (No  tocar)  WORK_DIR  =  Path(os.getcwd())  LLAMA_DIR  =  WORK_DIR  /  "llama.cpp"   #  ==========================================  #  üõ†  FUNCIONES  DEL  PROCESO  #  ==========================================   def  detectar_entorno():      """Verifica  que  est√°s  en  un  Mac."""      print(f"\n üñ•   Detectando  sistema...")      if  platform.system()  !=  "Darwin":          print(" ‚ùå  Este  script  est√°  dise√±ado  espec√≠ficamente  para  Mac  (Apple  Silicon).")          sys.exit(1)      print(" ‚úÖ  Sistema  Mac  detectado.  Usando  aceleraci√≥n  MLX  (GPU/Neural  Engine).")   def  preparar_datos():      """      Convierte  tus  ejemplos  de  arriba  en  archivos  .jsonl  que  MLX  entiende.      Genera  train.jsonl  (para  estudiar)  y  valid.jsonl  (para  examinarse).      """      print(f"\n üìÇ  [1/6]  Preparando  los  datos...")      os.makedirs(WORK_DIR  /  "data",  exist_ok=True)           datos_formateados  =  []      for  ej  in  EJEMPLOS_ENTRENAMIENTO:          #  Formato  ChatML  (System  ->  User  ->  Assistant)          msgs  =  [              {"role":  "system",  "content":  SYSTEM_PROMPT},              {"role":  "user",  "content":  ej["user"]},              {"role":  "assistant",  "content":  ej["assistant"]}          ]          datos_formateados.append({"messages":  msgs})   

    #  MLX  es  estricto:  necesita  validaci√≥n.  Duplicamos  los  datos  para  este  ejemplo  simple.      for  archivo  in  ["train.jsonl",  "valid.jsonl"]:          ruta  =  WORK_DIR  /  "data"  /  archivo          with  open(ruta,  "w",  encoding="utf-8")  as  f:              for  linea  in  datos_formateados:                  f.write(json.dumps(linea,  ensure_ascii=False)  +  "\n")                       print(f" ‚úÖ  Datos  listos  en  la  carpeta  ./data")   def  preparar_herramientas():      """      Descarga  y  compila  llama.cpp,  la  herramienta  necesaria  para  convertir  el  modelo.      Usa  'cmake'  porque  el  sistema  antiguo  'make'  da  problemas  en  versiones  nuevas.      """      print(f"\n üõ†   [2/6]  Preparando  herramientas  (llama.cpp)...")           #  1.  Clonar  si  no  existe      if  not  LLAMA_DIR.exists():          print("    ‚¨á   Clonando  repositorio...")          subprocess.run(["git",  "clone",  "https://github.com/ggerganov/llama.cpp.git"],  
check=True)
              #  2.  Instalar  dependencias  de  Python      print("    üì¶  Instalando  librer√≠as  auxiliares...")      try:          subprocess.run([sys.executable,  "-m",  "pip",  "install",  "-r",  "requirements.txt"],  
cwd=LLAMA_DIR,
 
check=True)
     except:          #  Si  falla  requirements.txt  (por  versiones  estrictas),  instalamos  lo  b√°sico  manual          print("    ‚ö†   Instalando  dependencias  manualmente  (modo  compatible)...")          subprocess.run([sys.executable,  "-m",  "pip",  "install",  "numpy",  "sentencepiece",  "gguf",  
"protobuf"],
 
check=True)
      #  3.  Compilar  con  CMake  (El  paso  cr√≠tico  que  fallaba  antes)      binario_quantize  =  LLAMA_DIR  /  "build"  /  "bin"  /  "llama-quantize"           if  not  binario_quantize.exists():          print("    üî®  Compilando  binarios  con  CMake  (esto  puede  tardar  unos  minutos)...")          try:              #  Crear  carpeta  de  construcci√≥n              (LLAMA_DIR  /  "build").mkdir(exist_ok=True)              #  Configurar  y  Construir              subprocess.run(["cmake",  "-B",  "build"],  cwd=LLAMA_DIR,  check=True)              subprocess.run(["cmake",  "--build",  "build",  "--config",  "Release",  "-j"],  
cwd=LLAMA_DIR,
 
check=True)
         except  Exception  as  e:              print(f" ‚ùå  Error  compilando:  {e}")              print(" üí°  Pista:  Aseg√∫rate  de  tener  cmake  instalado  ('brew  install  cmake')")  

            sys.exit(1)                   return  LLAMA_DIR   def  entrenar_y_fusionar():      """      Usa  la  librer√≠a  MLX  de  Apple  para  entrenar  los  adaptadores  LoRA  y  luego  fusionarlos.      """      print(f"\n üß†  [3/6]  Entrenando  modelo  (Fine-Tuning)...")      adapter_path  =  WORK_DIR  /  "adapters"           #  Comando  de  entrenamiento      #  Usamos  'python  -m  mlx_lm.lora'      cmd_train  =  [          sys.executable,  "-m",  "mlx_lm.lora",          "--model",  MODELO_ID,          "--train",           "--data",  str(WORK_DIR  /  "data"),          "--iters",  str(TRAIN_EPOCHS),          "--batch-size",  str(BATCH_SIZE),          "--adapter-path",  str(adapter_path)      ]           #  Si  ya  existen  adaptadores,  no  re-entrenamos  para  ahorrar  tiempo  (b√≥rralos  si  quieres  
empezar
 
de
 
cero)
     if  not  adapter_path.exists():          subprocess.run(cmd_train,  check=True)      else:          print("    ‚è©  Adaptadores  encontrados.  Saltando  entrenamiento.")       print(f"\n üîó  [4/6]  Fusionando  cerebro  nuevo  con  el  base...")      model_fused  =  WORK_DIR  /  f"{NOMBRE_PROYECTO}-Fused"           cmd_fuse  =  [          sys.executable,  "-m",  "mlx_lm.fuse",          "--model",  MODELO_ID,          "--adapter-path",  str(adapter_path),          "--save-path",  str(model_fused)      ]      subprocess.run(cmd_fuse,  check=True)           return  model_fused   def  reparar_diccionario(ruta_modelo_fusionado):      """      EL  TRASPLANTE:  Descarga  los  archivos  de  configuraci√≥n  originales  y  sanos      para  sobrescribir  los  que  MLX  genera  a  veces  corruptos.      Esto  evita  el  error  'list  object  has  no  attribute  keys'.  

    """      print(f"\n üöë  [5/6]  Auto-reparaci√≥n  del  Tokenizer...")      print("    Descargando  configuraci√≥n  original  sana  desde  HuggingFace...")           try:          snapshot_download(              repo_id=MODELO_ID,              allow_patterns=["tokenizer*",  "vocab*",  "merges*",  "special_tokens*"],              local_dir=ruta_modelo_fusionado,              local_dir_use_symlinks=False          )          print(" ‚úÖ  Reparaci√≥n  completada.  El  modelo  est√°  listo  para  convertirse.")      except  Exception  as  e:          print(f" ‚ö†  Error  en  la  reparaci√≥n:  {e}")   def  convertir_y_comprimir(ruta_modelo_fusionado,  llama_dir):      """      Convierte  a  GGUF  y  comprime  el  archivo  final.      """      print(f"\n üì¶  [6/6]  Exportando  a  GGUF  final...")           script_convert  =  llama_dir  /  "convert_hf_to_gguf.py"      archivo_f16  =  WORK_DIR  /  f"{NOMBRE_PROYECTO}.gguf"      archivo_final  =  WORK_DIR  /  f"{NOMBRE_PROYECTO}-{TIPO_CUANTIZACION}.gguf"           #  Paso  A:  Conversi√≥n  a  GGUF  crudo  (F16)      print("    A)  Convirtiendo  estructura  (F16)...")      subprocess.run([sys.executable,  str(script_convert),  str(ruta_modelo_fusionado),  
"--outfile",
 
str(archivo_f16),
 
"--outtype",
 
"f16"],
 
check=True)
          #  Paso  B:  Cuantizaci√≥n  (Compresi√≥n)  usando  el  binario  compilado      print(f"    B)  Comprimiendo  a  {TIPO_CUANTIZACION}...")      binario_quantize  =  llama_dir  /  "build"  /  "bin"  /  "llama-quantize"           subprocess.run([str(binario_quantize),  str(archivo_f16),  str(archivo_final),  
TIPO_CUANTIZACION],
 
check=True)
          #  Limpieza:  Borrar  el  archivo  intermedio  pesado      if  archivo_f16.exists():          os.remove(archivo_f16)               return  archivo_final   #  ==========================================  #  üèÅ  PUNTO  DE  ENTRADA  #  ==========================================  if  __name__  ==  "__main__":      try:  

        detectar_entorno()          preparar_datos()          llama_path  =  preparar_herramientas()                   #  Entrenamos  y  obtenemos  la  ruta  de  la  carpeta  fusionada          ruta_fused  =  entrenar_y_fusionar()                   #  Aplicamos  el  parche  de  reparaci√≥n  ANTES  de  convertir          reparar_diccionario(ruta_fused)                   #  Convertimos          ruta_final  =  convertir_y_comprimir(ruta_fused,  llama_path)                   print("\n"  +  "="*60)          print(f" üéâ  ¬°√âXITO  TOTAL!  Tu  IA  est√°  lista.")          print(f" üìÑ  Archivo:  {ruta_final}")          print(" üëâ  Arr√°stralo  a  LM  Studio  y  pru√©balo.")          print("="*60)               except  Exception  as  e:          print(f"\n ‚ùå  Ocurri√≥  un  error:  {e}")          print("Revisa  los  mensajes  anteriores  para  ver  qu√©  fall√≥.")     -  Si  estaba  en  Windows:   El  script  descarga  un  ZIP  oficial  de  llama.cpp,  extrae  el  archivo  llama-quantize.exe y  
lo
 
col√≥calo
 
en
 
la
 
carpeta.
  Ahora  toca  ejecutar  el  archivo:  python3  crear_llm_mac.py   
Parte  3:  Explicaci√≥n  Detallada  de  los  
Comandos
 
y
 
Opciones
 
Aqu√≠  explico  qu√©  hace  cada  secci√≥n  del  script  para  que  entiendas  la  "magia"  detr√°s,  
alineado
 
con
 
tu
 
petici√≥n.
 
1.  Configuraci√≥n  y  Dataset  ‚óè  MODELO_BASE  =  "mlx-community/Llama-3.2-3B...":  He  elegido  la  versi√≥n  Llama  
3.2
 
de
 
3
 
billones
 
de
 
par√°metros.
 ‚óã  Por  qu√©:  Es  un  modelo  muy  moderno  (2024),  peque√±o  (ideal  para  empezar),  
y
 
la
 
versi√≥n
 mlx-community ya  viene  preparada  para  Apple  Silicon.  ‚óè  crear_dataset():  Genera  el  archivo  train.jsonl.  ‚óã  Formato:  Usa  el  est√°ndar  de  mensajes  (role:  system/user/assistant)  que  es  el  
formato
 
moderno
 
de
 
chat.
 
Esto
 
es
 
crucial
 
para
 
que
 
el
 
modelo
 
entienda
 
que
 
es
 
una
 
conversaci√≥n.
 

2.  Entrenamiento  (mlx_lm.lora)  El  script  ejecuta  este  comando  internamente.  Desglosemos  las  opciones  importantes:  
‚óè  --model:  El  modelo  base  que  descargar√°  de  Hugging  Face.  ‚óè  --train:  Activa  el  modo  entrenamiento.  ‚óè  --iters  100:  Iteraciones .  Es  el  n√∫mero  de  veces  que  el  modelo  ver√°  ejemplos.  ‚óã  Nota:  Para  pruebas  r√°pidas,  100  est√°  bien.  Para  un  modelo  "inteligente"  real,  
sube
 
esto
 
a
 
600
 
o
 
1000
 
en
 
el
 
script.
 ‚óè  --batch-size  1:  Cu√°ntos  ejemplos  procesa  a  la  vez.  En  Mac,  mantenerlo  en  1  o  2  
ahorra
 
memoria
 
RAM
 
unificada.
 ‚óè  --lora-layers  16:  Cu√°ntas  capas  del  cerebro  estamos  adaptando.  16  es  un  est√°ndar  
equilibrado
 
entre
 
calidad
 
y
 
velocidad.
 ‚óè  --adapter-path:  D√≥nde  se  guardan  los  archivos  temporales  (.safetensors)  con  el  
nuevo
 
aprendizaje.
 
3.  Fusi√≥n  (mlx_lm.fuse)  ‚óè  Este  paso  toma  el  modelo  original  (que  pesa  varios  GBs)  y  le  "pega"  
matem√°ticamente
 
los
 
cambios
 
que
 
aprendi√≥
 
(los
 
adaptadores).
 
El
 
resultado
 
es
 
una
 
carpeta
 
con
 
un
 
modelo
 
completo
 
de
 
PyTorch/Safetensors.
 
4.  Cuantizaci√≥n  y  GGUF  (llama.cpp)  Aqu√≠  es  donde  hacemos  la  magia  para  LM  Studio.  
‚óè  Clonaci√≥n  y  Make:  El  script  descarga  llama.cpp (la  herramienta  est√°ndar  para  
correr
 
LLMs
 
en
 
local)
 
y
 
la
 
compila
 
usando
 make.  Al  estar  en  Mac,  detectar√°  
autom√°ticamente
 Metal (la  tecnolog√≠a  gr√°fica  de  Apple)  para  acelerar  el  proceso.  ‚óè  convert_hf_to_gguf.py:  Es  el  script  de  Python  dentro  de  llama.cpp  que  transforma  el  
modelo.
 ‚óè  --outtype  q4_k_m:  Esto  es  clave.  ‚óã  Significa  "Quantization  4-bit  K-Medium".  ‚óã  Reduce  el  peso  del  modelo  dr√°sticamente  (de  16  bits  a  4  bits  por  "neurona")  
manteniendo
 
casi
 
toda
 
la
 
inteligencia.
 
Es
 
el
 
formato
 
recomendado
 
por
 
LM
 
Studio.
 
Parte  4:  Integraci√≥n  en  LM  Studio  Una  vez  que  el  script  termine  y  veas  el  mensaje  de  "¬°√âXITO!",  sigue  estos  pasos  finales  
mencionados
 
en
 
la
 
documentaci√≥n
 
:
 
1.  Abre  LM  Studio .  2.  Ve  a  la  pesta√±a  de  b√∫squeda  (lupa)  o  a  "My  Models".  3.  Arrastra  el  archivo  generado  (Chef-Bot-Pro-Q4_K_M.gguf)  directamente  a  la  interfaz  
de
 
LM
 
Studio,
 
o
 
col√≥calo
 
en
 
la
 
carpeta
 
de
 
modelos
 
del
 
programa.
 4.  Selecciona  el  modelo  en  la  barra  superior.  5.  System  Prompt:  Configura  el  prompt  del  sistema  igual  que  en  el  entrenamiento  
para
 
activar
 
su
 
personalidad:
 
"Eres
 
un
 
chef
 
experto.
 
Tu
 
objetivo
 
es
 
dar
 
recetas
 
precisas
 
basadas
 
en
 
ingredientes."
 6.  ¬°Preg√∫ntale  por  una  receta  con  ingredientes  que  no  estaban  en  el  dataset  para  ver  
c√≥mo
 
generaliza!
 

